{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Easy Softprompt Tuner",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4Gx1qy98kPC"
      },
      "source": [
        "#@markdown # Easy Softprompt Tuner\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "!git clone https://github.com/ve-forbryderne/mtj-softtuner\n",
        "!bash mtj-softtuner/install.sh\n",
        "\n",
        "!apt install aria2 -y\n",
        "\n",
        "from mtj_softtuner import BasicTrainer\n",
        "\n",
        "# Change this to an integer (e.g. 1) if you want trainer.data to persist after\n",
        "# the Colab runtime is restarted\n",
        "universe = None\n",
        "\n",
        "# Changing this to True causes traceback of certain error messages to be hidden\n",
        "quiet = False\n",
        "\n",
        "trainer = BasicTrainer(universe, quiet=quiet)\n",
        "\n",
        "# Path to a Mesh Transformer JAX model, or the model ID of a Hugging Face model\n",
        "# such as \"KoboldAI/fairseq-dense-13B\"\n",
        "#@markdown Select a model or pick one from Huggingface (GPT-Neo, J and XGLM based models are supported)\n",
        "trainer.data.ckpt_path = \"togethercomputer/GPT-JT-6B-v1\" #@param [\"EleutherAI/gpt-j-6B\", \"KoboldAI/fairseq-dense-13B\", \"EleutherAI/gpt-neo-2.7B\", \"EleutherAI/gpt-neo-1.3B\", \"facebook/opt-13b\", \"facebook/opt-6.7b\"] {allow-input: true}\n",
        "trainer.get_hf_checkpoint_metadata()\n",
        "\n",
        "# Location of the save file (if the file does not exist it will be created), you\n",
        "# can specify the path to an existing save file created by mtj-softtuner to\n",
        "# continue from an earlier point in the training\n",
        "#@markdown Select a save for your prompt, you can leave this default but if you wish to start a new softprompt the file should not exist.\n",
        "trainer.data.save_file = \"/content/drive/MyDrive/newsoftprompt.mtjsp\" #@param[\"/content/drive/MyDrive/softprompt.mtjsp\"] {allow-input: true}\n",
        "\n",
        "# Set the initial soft prompt string, this will be ignored if we are continuing\n",
        "# from an existing save file\n",
        "#@markdown Optionally add the location of a training prompt txt containing a high quality snippet of your dataset around 70 words.\n",
        "trainingprompt = \"\" #@param [\"/content/drive/MyDrive/prompt.txt\", \"\"] {allow-input: true}\n",
        "if not trainingprompt:\n",
        "  trainer.data.prompt_method = \"vocab_sample\"\n",
        "  #@markdown In case you left the prompt blank you can specify here how many tokens your prompt takes up (Larger is more knowledge but takes up more space from the story context when you use your softprompt)\n",
        "  trainer.data.soft_in_dim = 128 #@param [\"20\", \"40\", \"60\", \"80\"] {type:\"raw\", allow-input: true}\n",
        "else:\n",
        "  with open(trainingprompt) as f:\n",
        "\t  initial_softprompt = f.read()\n",
        "\n",
        "  tokenizer = trainer.get_tokenizer()\n",
        "  if trainer.data.newlinemode == \"s\":  # Handle fairseq-style newlines if required\n",
        "      initial_softprompt = initial_softprompt.replace(\"\\n\", \"</s>\")\n",
        "  trainer.data.initial_softprompt = tokenizer.encode(\n",
        "      initial_softprompt, max_length=int(2e9), truncation=True\n",
        "  )\n",
        "\n",
        "# Do this to generate an NPY file for your dataset if you haven't already done so\n",
        "#@markdown You will need a dataset that contains your stories in plain text .txt files, adjust the location if neccesary (Unicode not supported, unix line endings recommended).\n",
        "dataset_path = \"/content/data.txt\"  #@param [\"/content/drive/MyDrive/dataset/\"] {allow-input: true}\n",
        "output_file = \"/content/dataset.npy\"\n",
        "#@markdown For 13B adjust the batch size to 400, everything else can remain 2048\n",
        "batch_size = 224 #@param [\"2048\", \"400\"] {type:\"raw\", allow-input: true}\n",
        "#@markdown For most use cases one epoch is enough, increase if you use a very small dataset.\n",
        "epochs =  256#@param {type:\"raw\", allow-input: true}\n",
        "trainer.tokenize_dataset(dataset_path, output_file, batch_size, epochs)\n",
        "\n",
        "dataset_file = output_file\n",
        "trainer.data.dataset_file = dataset_file\n",
        "#@markdown If you picked a small batch size increase this value accordingly (Suggested values are in the dropdown)\n",
        "trainer.data.gradient_accumulation_steps = 16 #@param [\"16\", \"64\"] {type:\"raw\", allow-input: true}\n",
        "\n",
        "# Set training hyperparameters here; see the demo notebook for explanation of\n",
        "# what these mean\n",
        "#@markdown Adjusting the learning rate effects how strongly the AI learns from the data, 3e-5 is a safe default. Only adjust if your softprompt breaks during training.\n",
        "learning_rate = 3e-5 #@param [\"3e-5\"] {type:\"raw\", allow-input: true}\n",
        "trainer.data.stparams = {\n",
        "    \"lr\": learning_rate,\n",
        "    \"max_grad_norm\": 10.0,\n",
        "    \"weight_decay\": 0.1,\n",
        "    \"warmup\": 0.1,\n",
        "    \"end_lr_multiplier\": 0.1,\n",
        "    \"save_every\": 10,\n",
        "}\n",
        "\n",
        "# Now, begin training!\n",
        "trainer.train()\n",
        "\n",
        "# Export to KoboldAI/mkultra format\n",
        "#@markdown And finally pick a name and description for your softprompt\n",
        "output_file = \"/content/drive/MyDrive/KoboldAI/softprompts/my_softprompt.zip\" #@param [\"/content/drive/MyDrive/my_softprompt.zip\"] {allow-input: true}\n",
        "name = \"NSFW Content Warning\" #@param [\"Your Softprompt Name Here\"] {allow-input: true}\n",
        "author = \"Julian Herrera/Puffy310@BirdL\" #@param [\"\"] {allow-input: true}\n",
        "supported = trainer.data.ckpt_path #param [\"Generic 2.7B\", \"Generic 6B\", \"Generic 13B\"] {allow-input: true}\n",
        "description = \"Serves as a base for content warning systems\" #@param [\"Your Softprompt Description Here\"] {allow-input: true}\n",
        "trainer.export_to_kobold(output_file, name, author, supported, description)\n",
        "output_file = \"/content/drive/MyDrive/my_softprompt.json\"\n",
        "soft_prompt_name = name\n",
        "soft_prompt_description = supported + \" - \" + description\n",
        "trainer.export_to_mkultra(output_file, soft_prompt_name, soft_prompt_description)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}